# 全自动无限采集 URL 工具

## 项目简介

### 目前只完成获取种子 URL 的功能，后续继续开发无限挖掘：

本项目旨在开发一个全自动、无限采集 URL 的工具，能够持续抓取指定网站（如 [https://dns.aizhan.com/](https://dns.aizhan.com/)）的URL（后续会补充更多途径），并进行去重处理。用户可自定义抓取间隔时间，以满足不同需求。

## 功能特性

- **持续抓取**：自动定期抓取指定网站的域名信息。
- **去重处理**：确保收集到的域名唯一，避免重复。
- **用户自定义间隔**：用户可设置抓取频率，控制抓取间隔时间。
- **数据存储**：将抓取到的域名存储在本地文件或数据库中，方便后续使用。

## 技术栈

- **编程语言**：Python
- **主要库**：
  - `requests`：发送 HTTP 请求，获取网页内容。
  - `BeautifulSoup`：解析 HTML，提取域名信息。
  - `argparse`：处理命令行参数，设置抓取间隔。
  - `time`：控制抓取频率，实现定时抓取。
  - `re`：正则表达式，用于匹配域名格式。

## 使用方法

1. **环境准备**：
   - 确保已安装 Python 3.x。
   - 安装项目依赖：
     ```bash
     pip install requests beautifulsoup4
     ```

2. **运行脚本**：
   - 克隆本仓库或下载 `main.py` 文件。
   - 在终端中运行脚本：
     ```bash
     python main.py
     ```
   - 程序会提示输入抓取间隔时间（单位：秒），默认值为 60 秒。输入后，程序将按照设置的间隔持续抓取域名信息。

## 示例
用户输入抓取间隔时间后，程序将开始抓取并显示获取到的域名列表。
![image](https://github.com/user-attachments/assets/7816f47d-45be-4dd4-8a17-cc3c6afdd7a8)


## 后续开发计划
无限挖掘功能：在现有基础上，增加从获取到的种子 URL 继续抓取新 URL 的功能，实现无限挖掘。

多线程支持：引入多线程技术，提高抓取效率，缩短抓取时间。

代理池集成：使用代理池，随机使用代理 IP，降低被目标网站封禁的风险。

错误重试机制：对于请求失败的情况，自动进行多次重试，确保数据的完整性。

动态网页处理：使用 Selenium 等工具处理需要渲染的动态网页，确保获取到完整的网页内容。

数据库集成：将抓取到的 URL 存储到数据库（如 MySQL、MongoDB），便于管理和查询。

数据分析与统计：对抓取的数据进行分析，生成统计报告，如域名分布、抓取成功率等。

Web 界面开发：开发简洁的 Web 界面，显示抓取进度、统计信息，支持手动控制抓取任务。

通知功能：抓取完成或发生错误时，通过邮件或短信通知用户。

## 注意事项
遵守网站抓取规则：在抓取过程中，遵守目标网站的 robots.txt 文件规定，尊重网站的抓取规则。

控制抓取频率：设置合理的抓取间隔，避免对目标网站造成过大负载。

数据存储：建议将抓取到的数据存储在数据库中，以便后续处理和分析。

法律合规：确保抓取行为符合当地法律法规，不侵犯他人权益。

## 贡献指南
欢迎提交 Issues 或 Pull Requests，参与项目的改进与完善。提交代码前，请确保代码通过了基本的测试，并遵循项目的编码规范。
